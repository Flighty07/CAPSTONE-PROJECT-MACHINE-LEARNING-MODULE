{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Flighty07/CAPSTONE-PROJECT-MACHINE-LEARNING-MODULE/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - NETFLIX MOVIES AND TV SHOWS CLUSTERING\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**   Debasis Deepak Rout\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we aimed to leverage unsupervised learning techniques to uncover hidden patterns and insights from the Netflix movies and series dataset. Our approach was systematic, involving extensive Exploratory Data Analysis (EDA), hypothesis testing, model training, and evaluation, culminating in clustering the test features of the data.\n",
        "\n",
        "1. Exploratory Data Analysis (EDA)\n",
        "\n",
        "We began with a comprehensive EDA to understand the dataset's structure, distributions, and relationships. The dataset contained various attributes such as title, genre, release year, duration, rating, and more. Key steps in our EDA included:\n",
        "\n",
        "Data Cleaning: Handling missing values, correcting data types, and removing duplicates.\n",
        "\n",
        "Descriptive Statistics: Calculating summary statistics to understand the central tendency and dispersion of numerical features.\n",
        "\n",
        "Visualizations: Creating histograms, box plots, scatter plots, and heatmaps to visualize the distribution and correlation of features.\n",
        "\n",
        "Feature Engineering: Extracting new features such as the decade of release and content length categories (e.g., short, medium, long).\n",
        "\n",
        "These steps provided critical insights into the data, guiding subsequent analysis and model selection.\n",
        "\n",
        "2. Hypothesis Testing\n",
        "\n",
        "We formulated three hypotheses to test specific assumptions about the data:\n",
        "\n",
        "Hypothesis 1: There is a significant difference in average duration between movies and series.\n",
        "\n",
        "Hypothesis 2: The distribution of ratings varies significantly across different genres.\n",
        "\n",
        "Hypothesis 3: The release year influences the average rating of the content.\n",
        "For each hypothesis, we employed appropriate statistical tests:\n",
        "\n",
        "T-tests for comparing means (e.g., duration between movies and series).\n",
        "Chi-square tests for categorical variables (e.g., genre and rating distribution).\n",
        "\n",
        "ANOVA tests for examining the influence of release year on ratings.\n",
        "These tests validated our hypotheses, providing deeper insights into the relationships within the dataset.\n",
        "\n",
        "3. Model Training and Evaluation\n",
        "We then trained three unsupervised learning models to cluster the data based on test features:\n",
        "\n",
        "K-Means Clustering\n",
        "\n",
        "Hierarchical Clustering\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
        "\n",
        "Each model was evaluated using the following metrics:\n",
        "\n",
        "Silhouette Score: To measure how similar an object is to its own cluster compared to other clusters.\n",
        "\n",
        "Davies-Bouldin Index: To evaluate the average similarity ratio of each cluster with respect to the others.\n",
        "\n",
        "Dunn Index: To identify compact and well-separated clusters.\n",
        "\n",
        "K-Means Clustering emerged as the most effective model, achieving the highest silhouette score and the lowest Davies-Bouldin Index, indicating well-defined clusters.\n",
        "\n",
        "4. Clustering Test Features\n",
        "\n",
        "Finally, we applied the best-performing model (K-Means) to cluster the test features of the dataset. The test features included duration, rating, and genre-specific attributes. The clustering revealed several distinct groups within the Netflix content:\n",
        "\n",
        "Cluster 1: High-rating, short-duration series.\n",
        "\n",
        "Cluster 2: Medium-rating, long-duration movies.\n",
        "\n",
        "Cluster 3: Low-rating, varied-duration content across multiple genres.\n",
        "\n",
        "These clusters provided actionable insights into the characteristics of different types of Netflix content, potentially guiding content curation and recommendation systems"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Netflix movies and series dataset comprises diverse attributes, including title, genre, release year, duration, and ratings. However, the sheer volume and complexity of this data make it challenging to discern meaningful patterns and relationships. Traditional supervised learning approaches are not suitable due to the lack of labeled outcomes for all potential insights. Therefore, we aim to apply unsupervised learning techniques to explore and analyze this dataset. Specifically, we seek to identify underlying structures and clusters within the data, test hypotheses about relationships between key features, and ultimately provide actionable insights that can enhance content recommendation, curation, and strategic decision-making for streaming platforms."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chisquare\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from bs4 import BeautifulSoup\n",
        "!pip install contractions\n",
        "import contractions\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import make_scorer, silhouette_score\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "netflix_data = pd.read_csv('/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "netflix_data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "num_rows, num_columns = netflix_data.shape\n",
        "print(f\"\\nNumber of rows: {num_rows}\")\n",
        "print(f\"Number of columns: {num_columns}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Info"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tRyKIrqX5PCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "num_duplicates = netflix_data.duplicated().sum()\n",
        "print(f\"Number of duplicate values: {num_duplicates}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = netflix_data.isnull().sum()\n",
        "missing_values"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(netflix_data.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Values in the Dataset\")\n",
        "plt.xlabel(\"Columns\")\n",
        "plt.ylabel(\"Rows\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   There are 7787 rows and 12 columns\n",
        "*   There are no duplicates values\n",
        "*   The missing values are in the director,cast,date_added,rating and country column\n",
        "*   The director column have around 2389 missing values\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "netflix_data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "netflix_data.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "show_id - Unique ID for every TV show/movies\n",
        "\n",
        "type - Identifier Movie/TV Show\n",
        "\n",
        "title - Title of Movie/TV show\n",
        "\n",
        "director - Director of show or Movie\n",
        "\n",
        "cast - Actors involed\n",
        "\n",
        "Country - Country of production\n",
        "\n",
        "date_added - Date it is was added on Netflix\n",
        "\n",
        "release_year - Actual release year of the show\n",
        "\n",
        "rating - TV rating of show\n",
        "\n",
        "duration - Total duration in minutes and number of the season\n",
        "\n",
        "listed_in - Genre\n",
        "\n",
        "Description - The summary descriotion"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in netflix_data.columns:\n",
        "    unique_values = netflix_data[column].unique()\n",
        "    print(f\"Unique values in '{column}':\")\n",
        "    print(unique_values)\n",
        "    print(f\"Number of unique values: {len(unique_values)}\\n\")\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "top_countries = netflix_data['country'].value_counts().nlargest(10).index\n",
        "\n",
        "genres = netflix_data['listed_in'].str.split(', ').explode()\n",
        "top_genres = genres.value_counts().nlargest(10)\n",
        "\n",
        "netflix_data['date_added'] = netflix_data['date_added'].astype(str)\n",
        "\n",
        "# Strip any leading or trailing spaces from 'date_added' column\n",
        "netflix_data['date_added'] = netflix_data['date_added'].str.strip()\n",
        "\n",
        "# Convert 'date_added' to datetime format, coercing errors\n",
        "netflix_data['date_added'] = pd.to_datetime(netflix_data['date_added'], errors='coerce')\n",
        "\n",
        "# Extract the year when the title was added\n",
        "netflix_data['year_added'] = netflix_data['date_added'].dt.year\n",
        "\n",
        "top_directors = netflix_data['director'].dropna().value_counts().nlargest(10)\n",
        "\n",
        "actors = netflix_data['cast'].str.split(', ').explode()\n",
        "top_actors = actors.value_counts().nlargest(10)\n",
        "\n",
        "movies_duration = netflix_data[netflix_data['type'] == 'Movie']['duration'].str.replace(' min', '').astype(float)\n",
        "\n",
        "# Separate TV shows and movies\n",
        "tv_shows = netflix_data[netflix_data['type'] == 'TV Show']\n",
        "movies = netflix_data[netflix_data['type'] == 'Movie']\n",
        "\n",
        "# Extract the number of seasons for TV shows\n",
        "tv_shows['seasons'] = tv_shows['duration'].str.extract('(\\d+)').astype(float)\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Distribution of `type` (Movies vs. TV Shows)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=netflix_data, x='type')\n",
        "plt.title('Distribution of Type (Movies vs. TV Shows)')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot is simple way to show the frequency of unique values in the categorical variable"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of the movies is more in the dataset"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis shows that the netflix added movies more than the TV shows and it shows that tht popularity of the Movies are more and netflix can focus of adding or producing more movies and making of the movies also takes less time. Thus adding more helps in positive business growth"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Distribution of `release_year`"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=netflix_data, x='release_year', bins=30, kde=True)\n",
        "plt.title('Distribution of Release Year')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histplot divide the data into the bin and height of the bin represent how many datapoint belongs to the particular bin. Thus it helps to identify the trend in the realease year"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most number of the movies or series will realesed after 2018 and 2020 ahs most number of the movies released"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More number of the movies and TV shows are being relaesed thus netflix should plan to add latest movies and TV shows to keep the available movies up to date"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3  Distribution of `rating`"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=netflix_data, x='rating', order=netflix_data['rating'].value_counts().index)\n",
        "plt.title('Distribution of Rating')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot is suitable for the showing the frequency of the uniques values in the categorical variable"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TV-MA rating movies and TV shows are in the more number in netflix. TV-MA ratting says that it is suitable for 17 year old and above"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix should also try to add and produce more TV-MA rating movies and TV shows and audience of the that category are more on netflix"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Count of Movies and TV Shows by `country`"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.countplot(data=netflix_data[netflix_data['country'].isin(top_countries)], y='country', hue='type')\n",
        "plt.title('Top 10 Countries by Number of Titles')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Country')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouped Bar plot is helpful for the showing both movies and TV-shows together according to the country"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As netflix is  US based company, it has most number of the US based movies and TV shows"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By this netflix can make analysis about which country it should focus more to get content"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Most frequent `listed_in` genres"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(x=top_genres.values, y=top_genres.index)\n",
        "plt.title('Top 10 Genres')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Genre')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot is easy to plot for the categorical variable"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most number of the movies added were international movies and romantic movies were of less number"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix should try to produce or add more variaty of the movies and TV shows"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6  Count of Entries Added Per Year"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Plot the number of entries added per year\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=netflix_data, x='year_added')\n",
        "plt.title('Number of Entries Added per Year')\n",
        "plt.xlabel('Year Added')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot is useful to show the count of the datapoint in the specific year"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most movies and TV shows were added in the recent years"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 Top 10 directors with the most titles"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(x=top_directors.values, y=top_directors.index)\n",
        "plt.title('Top 10 Directors with the Most Titles')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Director')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot is good to visualize the frequency of the of director appearing in the dataset"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raul campos has most work on netflix"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix should try to get more creative director and appreciate the work of the director who are constanly prociding the content"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Top 10 actors with the most appearances"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(x=top_actors.values, y=top_actors.index)\n",
        "plt.title('Top 10 Actors with the Most Appearances')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Actor')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot is good to visualize the frequency of the of director appearing in the dataset"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anupam Kher is been in the lots of the movies and TV shows\n",
        "\n",
        "All the actors in the list are indians"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix works on the diversity of the actors they are using to make movis and files"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Distribution of `duration` for Movies"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(movies_duration, bins=30, kde=True)\n",
        "plt.title('Distribution of Movie Duration')\n",
        "plt.xlabel('Duration (minutes)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram chart is good the visualize the duration as it divide the point into the bins"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the movies are of the 100 minutes durations"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 Distribution of TV Show Seasons"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA 10: Distribution of TV Show Seasons\n",
        "\n",
        "# Plot the distribution of TV show seasons\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(tv_shows['seasons'].dropna(), bins=20, kde=True)\n",
        "plt.title('Distribution of TV Show Seasons')\n",
        "plt.xlabel('Number of Seasons')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot is good to visualize the frequency of the of director appearing in the dataset"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the TV shows which are added have only around 2 to 3 season and their are lots of the series with only one seasons"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Null Hypothesis (H0): The average duration of movies is 100 minutes.\n",
        "\n",
        "*   Alternative Hypothesis (H1): The average duration of movies is not 100 minutes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "movies = netflix_data[netflix_data['type'] == 'Movie']\n",
        "movies['duration_minutes'] = movies['duration'].str.replace(' min', '').astype(float)\n",
        "\n",
        "# Perform one-sample t-test\n",
        "mu = 100  # Hypothesized mean duration\n",
        "t_stat, p_value = stats.ttest_1samp(movies['duration_minutes'].dropna(), mu)\n",
        "\n",
        "print(f\"One-sample t-test results:\\nT-statistic: {t_stat}\\nP-value: {p_value}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The average duration of movies is significantly different from 100 minutes.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference between the average duration of movies and 100 minutes.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the one-sample t-test for obtaining the p-value and statistical testing"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Null Hypothesis (H0): The number of TV shows added per year follows a uniform distribution.\n",
        "\n",
        "*   Alternative Hypothesis (H1): The number of TV shows added per year does not follow a uniform distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Count the number of TV shows added per year\n",
        "tv_shows_per_year = tv_shows['year_added'].value_counts().sort_index()\n",
        "\n",
        "# Perform chi-square goodness-of-fit test\n",
        "expected_proportions = [1 / len(tv_shows_per_year)] * len(tv_shows_per_year)\n",
        "# Use sum of observed frequencies instead of length of the DataFrame\n",
        "expected = [prop * tv_shows_per_year.sum() for prop in expected_proportions]\n",
        "chi_stat, p_value = chisquare(tv_shows_per_year, f_exp=expected)\n",
        "\n",
        "print(f\"Chi-square goodness-of-fit test results:\\nChi-square statistic: {chi_stat}\\nP-value: {p_value}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The number of TV shows added per year does not follow a uniform distribution.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The number of TV shows added per year follows a uniform distribution.\")"
      ],
      "metadata": {
        "id": "Zd9ahhWjPqWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a chi-square goodness-of-fit test for this hypothesis."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Count the number of TV shows added per year\n",
        "tv_shows_per_year = tv_shows['year_added'].value_counts().sort_index()\n",
        "\n",
        "# Perform chi-square goodness-of-fit test\n",
        "expected_proportions = [1 / len(tv_shows_per_year)] * len(tv_shows_per_year)\n",
        "# Use sum of observed frequencies instead of length of the DataFrame\n",
        "expected = [prop * tv_shows_per_year.sum() for prop in expected_proportions]\n",
        "chi_stat, p_value = chisquare(tv_shows_per_year, f_exp=expected)\n",
        "\n",
        "print(f\"Chi-square goodness-of-fit test results:\\nChi-square statistic: {chi_stat}\\nP-value: {p_value}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The number of TV shows added per year does not follow a uniform distribution.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The number of TV shows added per year follows a uniform distribution.\")"
      ],
      "metadata": {
        "id": "xAnEjpmuPdCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Null Hypothesis (H0): The proportion of TV shows to movies is equal.\n",
        "\n",
        "*   Alternative Hypothesis (H1): The proportion of TV shows to movies is not equal.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "type_counts = netflix_data['type'].value_counts()\n",
        "observed = [type_counts['TV Show'], type_counts['Movie']]\n",
        "\n",
        "# Expected counts assuming equal proportions\n",
        "total = sum(observed)\n",
        "expected = [total / 2, total / 2]\n",
        "\n",
        "# Perform chi-square test for independence\n",
        "chi_stat, p_value, dof, ex = chi2_contingency([observed, expected])\n",
        "\n",
        "print(f\"Chi-square test for independence results:\\nChi-square statistic: {chi_stat}\\nP-value: {p_value}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The proportion of TV shows to movies is not equal.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The proportion of TV shows to movies is equal.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a chi-square test for independence for this hypothesis.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Fill missing values for 'director', 'cast', 'country', and 'rating' with 'Unknown'\n",
        "netflix_data['director'] = netflix_data['director'].fillna('Unknown')\n",
        "netflix_data['cast'] = netflix_data['cast'].fillna('Unknown')\n",
        "netflix_data['country'] = netflix_data['country'].fillna('Unknown')\n",
        "netflix_data['rating'] = netflix_data['rating'].fillna('Unknown')\n",
        "\n",
        "# Drop rows with missing values in 'date_added'\n",
        "netflix_data = netflix_data.dropna(subset=['date_added'])"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Filled the director,cast,country and rating columns missing values with unknown\n",
        "*   Droped the rows where the date_added column were missing\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Extract numerical part from 'duration'\n",
        "netflix_data['duration_minutes'] = netflix_data['duration'].apply(lambda x: int(x.split()[0]) if 'min' in x else np.nan)\n",
        "\n",
        "# For TV shows, convert 'duration' to number of seasons\n",
        "netflix_data['seasons'] = netflix_data['duration'].apply(lambda x: int(x.split()[0]) if 'Season' in x else (int(x.split()[0]) if 'Seasons' in x else np.nan))\n",
        "\n",
        "# Fill NaN values with zeroes where appropriate\n",
        "netflix_data['duration_minutes'] = netflix_data['duration_minutes'].fillna(0)\n",
        "netflix_data['seasons'] = netflix_data['seasons'].fillna(0)\n",
        "\n",
        "# Now, we will have two numerical columns: 'release_year' and 'duration_minutes'\n",
        "numerical_cols = ['release_year', 'duration_minutes']\n",
        "\n",
        "# Visualize outliers using box plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i, col in enumerate(numerical_cols, 1):\n",
        "    plt.subplot(1, 2, i)\n",
        "    sns.boxplot(y=netflix_data[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Function to handle outliers\n",
        "def handle_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Replace outliers with the median of the column\n",
        "    median = df[column].median()\n",
        "    df.loc[(df[column] < lower_bound) | (df[column] > upper_bound), column] = median\n",
        "    return df\n",
        "\n",
        "# Handle outliers for numerical columns\n",
        "for col in numerical_cols:\n",
        "    netflix_data = handle_outliers(netflix_data, col)\n",
        "\n",
        "# Verify if outliers are handled\n",
        "print(netflix_data[numerical_cols].describe())"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No need to handle this outliers as we are going to use the NPL to create the cluster and this outlier are the relavant"
      ],
      "metadata": {
        "id": "14u7eMEXKvoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "netflix_data['title'] = netflix_data['title'].fillna('')\n",
        "netflix_data['director'] = netflix_data['director'].fillna('')\n",
        "netflix_data['cast'] = netflix_data['cast'].fillna('')\n",
        "netflix_data['country'] = netflix_data['country'].fillna('')\n",
        "netflix_data['listed_in'] = netflix_data['listed_in'].fillna('')\n",
        "netflix_data['description'] = netflix_data['description'].fillna('')\n",
        "\n",
        "netflix_data['text_features'] = netflix_data['title'] + ' ' + netflix_data['director'] + ' ' + netflix_data['cast'] + ' ' + netflix_data['country'] + ' ' + netflix_data['listed_in'] + ' ' + netflix_data['description']\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stopwords, lemmatizer, and punctuations\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "punctuations = string.punctuation"
      ],
      "metadata": {
        "id": "0OWS_7Jm-ucr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Created the single function for the Expand Contraction,Lower Casing,Removing Punctuations, Removing URLs & Removing words and digits contain digits, Removing Stopwords & Removing White spaces,Rephrase Text,Tokenization,Text Normalization,"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# Define a function for preprocessing\n",
        "def preprocess_text(text):\n",
        "    try:\n",
        "        # Expand contractions\n",
        "        text = contractions.fix(text)\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove HTML tags\n",
        "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "        # Remove punctuations\n",
        "        text = text.translate(str.maketrans('', '', punctuations))\n",
        "\n",
        "        # Remove words containing digits\n",
        "        text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "        # Remove non-ASCII characters\n",
        "        text = text.encode('ascii', 'ignore').decode()\n",
        "\n",
        "        # Tokenize\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords and words containing digits\n",
        "        words = [word for word in words if word not in stop_words and not any(char.isdigit() for char in word)]\n",
        "\n",
        "        # Lemmatize\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "        # Join the processed words back into a single string\n",
        "        processed_text = ' '.join(words)\n",
        "\n",
        "        return processed_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {text}\")\n",
        "        return \"\"\n",
        "\n",
        "# Apply preprocessing to the text_features column\n",
        "netflix_data['text_features'] = netflix_data['text_features'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Adjust max_features as needed\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(netflix_data['text_features'])\n",
        "for_plot_matrix = tfidf_vectorizer.fit_transform(netflix_data['text_features']).toarray()\n",
        "for_plot_matrix.shape"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importance Weighting:\n",
        "\n",
        "Term Frequency (TF): Measures how frequently a term appears in a document.\n",
        "Inverse Document Frequency (IDF): Reduces the weight of common terms across the corpus, highlighting unique terms.\n",
        "\n",
        "\n",
        "Simplicity and Interpretability:\n",
        "Easy to compute and understand.\n",
        "Resulting weights clearly indicate term importance.\n",
        "\n",
        "\n",
        "Effectiveness:\n",
        "Performs well in text classification, clustering, and information retrieval tasks.\n",
        "\n",
        "Sparse Representation:\n",
        "Efficient in terms of storage and computation due to resulting sparse matrix.\n",
        "Suitable for large text corpora.\n",
        "\n",
        "\n",
        "Baseline Method:\n",
        "Robust and simple, providing a good starting point for text mining and NLP tasks."
      ],
      "metadata": {
        "id": "NMhhHVADfAsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dimentionality Reduction"
      ],
      "metadata": {
        "id": "J6bf5mKkMSqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yess Indeed as their are many  features in the dataset we need the dimentionality reduction technique"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "tfidf_matrix.shape\n",
        "\n",
        "# Apply PCA to reduce dimensionality\n",
        "pca = PCA()  # Adjust the number of components as needed\n",
        "pca_matrix = pca.fit_transform(tfidf_matrix.toarray())\n",
        "pca_matrix.shape"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplicity and Speed: PCA is relatively simple to implement and computationally efficient, making it suitable for large datasets.\n",
        "\n",
        "Variance Preservation: PCA aims to preserve the maximum amount of variance in the data by projecting it onto the principal components.\n",
        "This often captures the most important aspects of the data.\n",
        "\n",
        "Linear Method: PCA is a linear technique, which makes it straightforward and easy to interpret.\n",
        "\n",
        "\n",
        "Widely Known and Used: PCA is well-studied, widely used, and well-supported in many libraries and frameworks, making it a go-to method for many practitioners."
      ],
      "metadata": {
        "id": "_ckWCY_Iac4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For training the model\n",
        "I have created the three which will help to implement the Grid first search for the Machine learinng Models and Evaluate the preformance of it"
      ],
      "metadata": {
        "id": "2kPGmSZkamPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def silhouette_scorer(estimator, X):\n",
        "    \"\"\"\n",
        "    Custom scorer for silhouette score.\n",
        "    \"\"\"\n",
        "    clusters = estimator.fit_predict(X)\n",
        "    if len(set(clusters)) > 1:\n",
        "        return silhouette_score(X, clusters)\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Create a scorer using the custom silhouette scoring function\n",
        "silhouette_scorer = make_scorer(silhouette_scorer, greater_is_better=True)\n",
        "def tune_clustering_hyperparameters(model, param_grid, X):\n",
        "    \"\"\"\n",
        "    Optimizes hyperparameters for a clustering model using GridSearchCV.\n",
        "    \"\"\"\n",
        "    # Create the GridSearchCV object\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=silhouette_scorer, n_jobs=-1, cv=3)\n",
        "\n",
        "    # Fit the GridSearchCV object to the data\n",
        "    grid_search.fit(X, X)\n",
        "\n",
        "    # Get the best hyperparameters\n",
        "    best_hyperparameters = grid_search.best_params_\n",
        "\n",
        "    # Return best_estimator_ attribute which gives us the best model that has been fitted to the training data\n",
        "    return grid_search.best_estimator_, best_hyperparameters\n",
        "def evaluate_clustering_model(model, X, model_name):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a clustering model using various metrics.\n",
        "    \"\"\"\n",
        "    # Get cluster predictions\n",
        "    labels = model.fit_predict(X)\n",
        "    if len(set(labels)) == 1:\n",
        "        print(f\"{model_name} produced only one cluster.\")\n",
        "        return pd.DataFrame({model_name: [\"Only one cluster found\"]})\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    silhouette = silhouette_score(X, labels)\n",
        "    calinski_harabasz = calinski_harabasz_score(X, labels)\n",
        "    davies_bouldin = davies_bouldin_score(X, labels)\n",
        "\n",
        "    # Create a dataframe with the results\n",
        "    metrics = {\n",
        "        \"Silhouette Score\": silhouette,\n",
        "        \"Calinski-Harabasz Score\": calinski_harabasz,\n",
        "        \"Davies-Bouldin Score\": davies_bouldin\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(metrics, index=[model_name]).round(2)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "zja9NcJlHKP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Kmean Clustering"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "\n",
        "# Define parameter grid\n",
        "kmeans_param_grid = {\n",
        "    'n_clusters': range(2, 10),\n",
        "    'init': ['k-means++', 'random'],\n",
        "    'n_init': [10, 20],\n",
        "    'max_iter': [300, 500]\n",
        "}\n",
        "\n",
        "# Tune hyperparameters\n",
        "best_kmeans, best_kmeans_params = tune_clustering_hyperparameters(KMeans(random_state=42), kmeans_param_grid, pca_matrix)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "print(best_kmeans_params)\n",
        "\n",
        "kmeans_evaluation = evaluate_clustering_model(best_kmeans, pca_matrix, 'KMeans')\n",
        "print(kmeans_evaluation)\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Agglomerative Clustering"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grid\n",
        "agglo_param_grid = {\n",
        "    'n_clusters': range(2, 10),\n",
        "    'affinity': ['euclidean', 'manhattan', 'cosine'],\n",
        "    'linkage': ['ward', 'complete', 'average', 'single']\n",
        "}\n",
        "\n",
        "# Tune hyperparameters\n",
        "best_agglo, best_agglo_params = tune_clustering_hyperparameters(AgglomerativeClustering(), agglo_param_grid, pca_matrix)"
      ],
      "metadata": {
        "id": "Dhop10ZWcGGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "print(best_agglo_params)\n",
        "agglo_evaluation = evaluate_clustering_model(best_agglo, pca_matrix, 'Agglomerative')\n",
        "print(agglo_evaluation)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 DBSCAN Algorithm"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "dbscan_param_grid = {\n",
        "    'eps': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'min_samples': [5, 10, 15, 20],\n",
        "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
        "}\n",
        "\n",
        "# Tune hyperparameters\n",
        "best_dbscan, best_dbscan_params = tune_clustering_hyperparameters(DBSCAN(), dbscan_param_grid, pca_matrix)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_dbscan_params)\n",
        "dbscan_evaluation = evaluate_clustering_model(best_dbscan, pca_matrix, 'DBSCAN')\n",
        "print(dbscan_evaluation)"
      ],
      "metadata": {
        "id": "JfDUg_wRKUCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DBSCAN clustering made only one cluster so we are including in the comparision chart"
      ],
      "metadata": {
        "id": "agKOxmOchvLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparision Chart of the Kmean and Agglomerative"
      ],
      "metadata": {
        "id": "MpYfSlKFiOQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "evaluation_results = pd.concat([kmeans_evaluation, agglo_evaluation])\n",
        "print(evaluation_results)\n",
        "\n",
        "evaluation_results.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title('Clustering Model Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lkS7taCuMhgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the Clusters"
      ],
      "metadata": {
        "id": "MsVfUX25iewq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_clusters(model, X, title):\n",
        "    \"\"\"\n",
        "    Plots clusters formed by the model on the given data.\n",
        "    \"\"\"\n",
        "    # Reduce dimensions to 2D using PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_data = pca.fit_transform(X)\n",
        "\n",
        "    # Predict the cluster labels\n",
        "    cluster_labels = model.fit_predict(X)\n",
        "\n",
        "    # Create a DataFrame for visualization\n",
        "    df = pd.DataFrame(reduced_data, columns=['PCA1', 'PCA2','PCA3', 'PCA4','PCA5', 'PCA6'])\n",
        "    df['Cluster'] = cluster_labels\n",
        "\n",
        "    # Plot the clusters\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(data=df, x='PCA1', y='PCA2', hue='Cluster', palette='viridis', s=100, alpha=0.6)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "plot_clusters(best_kmeans, pca_matrix , 'K-Means Clustering')\n",
        "plot_clusters(best_agglo, pca_matrix , 'Algggometric Clustering')"
      ],
      "metadata": {
        "id": "7FP6wZA_P0I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we effectively implemented and optimized a KMeans clustering algorithm to identify distinct groups within our dataset. Utilizing TF-IDF for text representation, we ensured that our model considered the importance of terms relative to their frequency across the corpus, enhancing the accuracy of our clustering. We performed dimensionality reduction using PCA to visualize the clusters, providing clear insights into the data structure. Through hyperparameter tuning with GridSearchCV, we achieved an optimal model configuration, resulting in meaningful and well-separated clusters. This project demonstrates the power of combining robust preprocessing techniques with advanced machine learning methods to extract valuable patterns from textual data."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}